{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import machine learning and dataset modules\n",
    "import torch\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable PyTorch to use CUDA cores if they are avialable\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToFractionTensor:\n",
    "    # Convert ndarrays to Tensors\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        s = inputs[0].sum()\n",
    "        \n",
    "        inputs = np.divide(inputs, s)\n",
    "        \n",
    "        inputs[inputs < 0] = -1\n",
    "        \n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement custom dataset loader for synthetic data\n",
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, tar_path='datasets/synthetic.tar.gz', transform=None):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        with tarfile.open(tar_path) as tar:\n",
    "            for member in tar.getmembers():\n",
    "                if member.isfile():\n",
    "                    f = tar.extractfile(member)\n",
    "                    fn = member.name\n",
    "                    bn = os.path.basename(fn)\n",
    "                    uuid = os.path.basename(Path(fn).parent)\n",
    "                    df = pd.read_csv(f)\n",
    "                    \n",
    "                    # If filename is features.csv, append to y tensor\n",
    "                    if 'features.csv' in bn:\n",
    "                        df['network_name'] = [os.path.splitext(name)[0] for name in df['network_name']]\n",
    "                        df.drop(['population', 'backend'], axis=1, inplace=True)\n",
    "                        self.y.append(df)\n",
    "                        \n",
    "                    # If filename is timeseries.csv, append to X tensor\n",
    "                    if 'timeseries.csv' in bn:\n",
    "                        df['uuid'] = uuid\n",
    "                        self.X.append(df)\n",
    "        \n",
    "        self.y = pd.concat(self.y)\n",
    "        self.X = pd.concat(self.X)\n",
    "        \n",
    "        self.network_mapping = {k: v for v, k in enumerate(self.y['network_name'].unique())}\n",
    "        self.y['network_type'] = self.y['network_name'].map(self.network_mapping)\n",
    "        \n",
    "        self.y_data = self.y.drop(['network_name'], axis=1).to_numpy(dtype=np.float64)\n",
    "        \n",
    "        # Stackoverflow magic\n",
    "        g = self.X.groupby('uuid').cumcount()\n",
    "        self.X_data = (self.X.set_index(['uuid', g])\n",
    "            .unstack(fill_value=-1).stack()\n",
    "            .groupby(level=0)\n",
    "            .apply(lambda x: x.values.tolist())\n",
    "            .tolist())\n",
    "        \n",
    "        self.X_data = np.array(self.X_data, dtype=np.int)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.X_data[index], self.y_data[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design the convolutional neural network model to determine network_type\n",
    "class ConvnetNetworkClassifier(nn.Module):\n",
    "    def __init__(self, n_input_features=500, n_output_features=1):\n",
    "        super(ConvnetNetworkClassifier, self).__init__()\n",
    "        self.n_input_features = n_input_features\n",
    "        self.n_output_features = n_output_features\n",
    "    \n",
    "    def forward(self, X):\n",
    "        pass\n",
    "\n",
    "    def shape(self):\n",
    "        return self.n_input_features, self.n_output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries(ts):\n",
    "    ts = pd.DataFrame(ts[~np.all(ts == -1, axis=1)],\n",
    "                      columns=['susceptible', 'infected', 'recovered', 'dead'])\n",
    "    ts = pd.DataFrame(ts, columns=['susceptible', 'infected', 'recovered', 'dead'])\n",
    "    ts.plot()\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('fraction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up training, validation, and testing dataset\n",
    "whole_set = SyntheticDataset(transform=ToFractionTensor())\n",
    "\n",
    "l = len(whole_set)\n",
    "\n",
    "# training = 50%, validation = 20%, testing = 30%\n",
    "n_val = int(0.2*l)\n",
    "n_test = int(0.3*l)\n",
    "n_train = l - n_test - n_val\n",
    "\n",
    "train_set, val_set, test_set = random_split(whole_set, [n_train, n_val, n_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset number\n",
    "DSN = 17\n",
    "\n",
    "# Plot data for sanity\n",
    "plot_timeseries(whole_set[DSN][0].numpy())\n",
    "\n",
    "# Print out parameters\n",
    "whole_set.y.iloc[DSN, :-1], whole_set.X_data.shape, whole_set.y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "test_set = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 28)"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of input and output features\n",
    "n_input_features = whole_set.X_data.shape[1]\n",
    "n_output_features = len(whole_set.network_mapping)\n",
    "\n",
    "# Create convolutional multiclassifier for network type\n",
    "model = ConvnetNetworkClassifier(n_input_features=n_input_features,\n",
    "                                 n_output_features=n_output_features).to(device)\n",
    "\n",
    "model.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
